{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob, os\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# Adding a target column\n",
    "def generateTarget(row) :\n",
    "    if row['trial_type'] == 'ADLs' :\n",
    "        return 0\n",
    "    if row['trial_type'] == 'Near_Falls' :\n",
    "        return 0\n",
    "    if row['trial_type'] == 'Falls' :\n",
    "        return 1\n",
    "    \n",
    "# Adding a target column\n",
    "def generateTarget2(row) :\n",
    "    return 0\n",
    "\n",
    "# We'll use this function to test our models from now on\n",
    "def modelProcessing(X_train,y_train,X_test,y_test,model) :\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "    specificity = (tn / (fp + tn))*100\n",
    "    sensitivity = (tp / (tp + fn))*100\n",
    "    accuracy = ((tn+tp) / (tp + tn + fp + fn))\n",
    "    print(\"Confusion matrix : \")\n",
    "    print(\"TN : \"+str(tn) + \" FP : \" +str(fp))\n",
    "    print(\"FN : \"+str(fn) + \" TP : \" +str(tp))\n",
    "    print(\"\")\n",
    "    print(\"Accuracy : \"+str(accuracy_score(y_test,y_pred)))\n",
    "    print(\"Recall : \" +str(recall_score(y_test,y_pred)))\n",
    "    print(\"Precision : \"+str(precision_score(y_test,y_pred)))\n",
    "    print(\"F-measure :\"+str(f1_score(y_test,y_pred)))\n",
    "    print(\"Sensitivity : \"+str(sensitivity))\n",
    "    print(\"Specificity : \"+str(specificity))\n",
    "    \n",
    "\n",
    "# Obtain a DF with the metrics and bodyparts you want\n",
    "def filterCols(df,metrics,bodyparts,resultants=True) :\n",
    "    # Metrics are Acceleration,Magnetic and Velocity (List of strings)\n",
    "    # Bodyparts are waist,l.ankle,r.ankle,l.thigh,r.thigh,sternum,head (list of strings)\n",
    "    # Resultants = true will get the resultants of the respective metrics\n",
    "    groupcols = ['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime']\n",
    "    metriccols = []\n",
    "    bodycols = []\n",
    "    \n",
    "    for col in df.columns.values :\n",
    "        for metric in metrics :\n",
    "            if (metric in col) :\n",
    "                metriccols.append(col)\n",
    "            if (resultants) :\n",
    "                if (metric.lower() in col) :\n",
    "                    metriccols.append(col)\n",
    "        for part in bodyparts :\n",
    "            if (part in col) :\n",
    "                bodycols.append(col)\n",
    "    dfOut = df[groupcols + list(set(metriccols) & set(bodycols))]\n",
    "    return dfOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full database\n",
    "dfMain = pickle.load(open(\"../../../dataResultants/dataset_consolidated.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run one of the following cells at a time unless your computer has enough RAM\n",
    "# Used for the overlapping window preprocessing\n",
    "df = dfMain.copy()\n",
    "df = dfMain.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the resultant peak window preprocessing\n",
    "df2 = dfMain.copy()\n",
    "df2 = df2.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the resultant difference window preprocessing\n",
    "df3 = dfMain.copy()\n",
    "df3 = df3.drop(['target','Time','time_seconds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the convolutional neural network approach\n",
    "# Used for the resultant peak window preprocessing\n",
    "df4 = dfMain.copy()\n",
    "df4 = df4.drop(['target','Time','time_seconds'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data columns and separate them based on the sensor and the feature (accel, vel and magfield)\n",
    "\n",
    "allcols = dfMain.columns.values\n",
    "\n",
    "groupcols = ['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime']\n",
    "waistcols = []\n",
    "ranklecols = []\n",
    "lanklecols = []\n",
    "rthighcols = []\n",
    "lthighcols = []\n",
    "headcols = []\n",
    "sternumcols = []\n",
    "accelcols = []\n",
    "velcols = []\n",
    "magcols = []\n",
    "meancols = []\n",
    "resultantcols = []\n",
    "varcols = []\n",
    "\n",
    "for col in allcols : \n",
    "    if 'r.ankle' in col :\n",
    "        ranklecols.append(col)\n",
    "    if 'l.ankle' in col :\n",
    "        lanklecols.append(col)\n",
    "    if 'waist' in col :\n",
    "        waistcols.append(col)\n",
    "    if 'r.thigh' in col :\n",
    "        rthighcols.append(col)\n",
    "    if 'l.thigh' in col :\n",
    "        lthighcols.append(col)\n",
    "    if 'head' in col :\n",
    "        headcols.append(col)\n",
    "    if 'Velocity' in col :\n",
    "        velcols.append(col)\n",
    "    if 'Magnetic' in col :\n",
    "        magcols.append(col)\n",
    "    if 'Acceleration' in col :\n",
    "        accelcols.append(col)\n",
    "    if 'mean' in col :\n",
    "        meancols.append(col)\n",
    "    if 'var' in col :\n",
    "        varcols.append(col)\n",
    "    if 'resultant' in col :\n",
    "        resultantcols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(resultantcols,axis=1) # Drop the resultant columns from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the mean\n",
    "df_window_mean = df.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).mean()\n",
    "df_window_mean = df_window_mean.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_mean' in the end\n",
    "for col in accelcols:\n",
    "    df_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)\n",
    "\n",
    "for col in velcols:\n",
    "    df_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)\n",
    "    \n",
    "for col in magcols:\n",
    "    df_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group in intervals of 2 seconds, calculating the variance\n",
    "\n",
    "df_window_variance = df.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).var()\n",
    "df_window_variance = df_window_variance.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_variance' in the end\n",
    "\n",
    "for col in accelcols : \n",
    "    df_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    \n",
    "for col in velcols : \n",
    "    df_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    \n",
    "for col in magcols : \n",
    "    df_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe, with all accelerometer columns (means and variances)\n",
    "all_trials_window = pd.merge(df_window_mean, df_window_variance,on=['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime'])\n",
    "\n",
    "# This dataframe will be used in case we decide to try different preprocessing steps\n",
    "all_trials_window = all_trials_window.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data columns and separate them based on the sensor and the feature (accel, vel and magfield)\n",
    "\n",
    "allcols = all_trials_window.columns.values\n",
    "\n",
    "groupcols = ['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original']\n",
    "waistcols = []\n",
    "ranklecols = []\n",
    "lanklecols = []\n",
    "rthighcols = []\n",
    "lthighcols = []\n",
    "headcols = []\n",
    "sternumcols = []\n",
    "accelcols = []\n",
    "velcols = []\n",
    "magcols = []\n",
    "meancols = []\n",
    "varcols = []\n",
    "\n",
    "for col in allcols : \n",
    "    if 'r.ankle' in col :\n",
    "        ranklecols.append(col)\n",
    "    if 'l.ankle' in col :\n",
    "        lanklecols.append(col)\n",
    "    if 'waist' in col :\n",
    "        waistcols.append(col)\n",
    "    if 'r.thigh' in col :\n",
    "        rthighcols.append(col)\n",
    "    if 'l.thigh' in col :\n",
    "        lthighcols.append(col)\n",
    "    if 'head' in col :\n",
    "        headcols.append(col)\n",
    "    if 'Velocity' in col :\n",
    "        velcols.append(col)\n",
    "    if 'Magnetic' in col :\n",
    "        magcols.append(col)\n",
    "    if 'Acceleration' in col :\n",
    "        accelcols.append(col)\n",
    "    if 'mean' in col :\n",
    "        meancols.append(col)\n",
    "    if 'var' in col :\n",
    "        varcols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overlapping windows around axis acceleration peaks\n",
    "#### First we find the biggest peak in each acceleration axis (be it a maximum or minimum peak) and we create a window that spans from the smallest peak-1 second to the biggest peak + 1 second. This creates a window that takes into account all 3 peaks. \n",
    "#### We'll be using only the waist for these tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject' 'trial_type' 'trial_subtype' 'trial_num' 'trial_num_original'\n",
      " 'time_datetime' 'waist Acceleration Z (m/s^2)_var'\n",
      " 'waist Acceleration X (m/s^2)_var' 'waist Acceleration Z (m/s^2)_mean'\n",
      " 'waist Acceleration X (m/s^2)_mean' 'waist Acceleration Y (m/s^2)_var'\n",
      " 'waist Acceleration Y (m/s^2)_mean']\n"
     ]
    }
   ],
   "source": [
    "# Get just waist acceleration columns, time and groupcols\n",
    "dfWaistAccels = filterCols(all_trials_window,['Acceleration'],['waist'],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auxdf = dfWaistAccels\n",
    "\n",
    "# Add absolute value of the acceleration means as new columns to auxdf\n",
    "auxdf['AbsX'] = auxdf['waist Acceleration X (m/s^2)_mean'].abs()\n",
    "auxdf['AbsY'] = auxdf['waist Acceleration Y (m/s^2)_mean'].abs()\n",
    "auxdf['AbsZ'] = auxdf['waist Acceleration Z (m/s^2)_mean'].abs()\n",
    "\n",
    "# Find the id of the rows with max absolute value for each axis\n",
    "dfWaistAccels['YMax'] = auxdf.groupby(groupcols)['AbsY'].transform('idxmax')\n",
    "dfWaistAccels['XMax'] = auxdf.groupby(groupcols)['AbsX'].transform('idxmax')\n",
    "dfWaistAccels['ZMax'] = auxdf.groupby(groupcols)['AbsZ'].transform('idxmax')\n",
    "\n",
    "# Find the max and min ids from the last section\n",
    "dfWaistAccels['AxisMax'] = dfWaistAccels[[\"YMax\", \"XMax\",\"ZMax\"]].max(axis=1)\n",
    "dfWaistAccels['AxisMin'] = dfWaistAccels[[\"YMax\", \"XMax\",\"ZMax\"]].min(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "dfList = []\n",
    "for sub in dfWaistAccels['subject'].unique() :\n",
    "    for trialtype in dfWaistAccels['trial_type'].unique() :\n",
    "        for subtype in dfWaistAccels['trial_subtype'].unique() :\n",
    "            for num in dfWaistAccels['trial_num'].unique() :\n",
    "                aux1 = dfWaistAccels[(dfWaistAccels['subject'] == sub) & (dfWaistAccels['trial_type'] == trialtype) \n",
    "                    & (dfWaistAccels['trial_subtype'] == subtype) & (dfWaistAccels['trial_num'] == num)]\n",
    "                aux2 = aux1[(aux1.index < aux1.AxisMax+2) & (aux1.index > aux1.AxisMin-2)]\n",
    "                dfList.append(aux2)\n",
    "\n",
    "fulldf = pd.concat(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns we don't need anymore\n",
    "fulldf = fulldf.drop(['XMax','ZMax','YMax','AxisMax','AxisMin','AbsX','AbsY','AbsZ'],axis=1)\n",
    "    \n",
    "fulldf['target'] = fulldf.apply (lambda row: generateTarget(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.780831914376\n",
      "Recall : 0.414669571532\n",
      "Precision : 0.857357357357\n",
      "F-measure :0.558981889378\n"
     ]
    }
   ],
   "source": [
    "y_train = fulldf[(fulldf['subject'] >= 6)]['target']\n",
    "X_train = fulldf[(fulldf['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', 'trial_num', 'time_datetime','target'],axis=1)\n",
    "y_test = fulldf[(fulldf['subject'] < 6)]['target']\n",
    "X_test = fulldf[(fulldf['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', 'trial_num', 'time_datetime','target'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using only mean columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.778156166383\n",
      "Recall : 0.366739288308\n",
      "Precision : 0.926605504587\n",
      "F-measure :0.525494276795\n"
     ]
    }
   ],
   "source": [
    "y_train = fulldf[(fulldf['subject'] >= 6)]['target']\n",
    "X_train = fulldf[(fulldf['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', 'trial_num', 'time_datetime','target'],axis=1)\n",
    "X_train = X_train.drop(['waist Acceleration X (m/s^2)_var','waist Acceleration Y (m/s^2)_var',\n",
    "                        'waist Acceleration Z (m/s^2)_var'],axis=1)\n",
    "y_test = fulldf[(fulldf['subject'] < 6)]['target']\n",
    "X_test = fulldf[(fulldf['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', 'trial_num', 'time_datetime','target'],axis=1)\n",
    "X_test = X_test.drop(['waist Acceleration X (m/s^2)_var','waist Acceleration Y (m/s^2)_var',\n",
    "                        'waist Acceleration Z (m/s^2)_var'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Resultant peak windows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the mean\n",
    "df2_window_mean = df2.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).mean()\n",
    "df2_window_mean = df2_window_mean.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_mean' in the end\n",
    "for col in df2.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df2_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the variance\n",
    "\n",
    "df2_window_variance = df2.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='500000us')]).var()\n",
    "df2_window_variance = df2_window_variance.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_variance' in the end\n",
    "for col in df2.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df2_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe, with all accelerometer columns (means and variances)\n",
    "df2_all_windows = pd.merge(df2_window_mean, df2_window_variance,on=['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime'])\n",
    "\n",
    "# This dataframe will be used in case we decide to try different preprocessing steps\n",
    "df2_all_windows = df2_all_windows.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject' 'trial_type' 'trial_subtype' 'trial_num' 'trial_num_original'\n",
      " 'time_datetime' 'waist Acceleration Y (m/s^2)_mean'\n",
      " 'waist Acceleration Y (m/s^2)_var' 'waist resultant acceleration_var'\n",
      " 'waist Acceleration Z (m/s^2)_var' 'waist Acceleration Z (m/s^2)_mean'\n",
      " 'waist Acceleration X (m/s^2)_mean' 'waist Acceleration X (m/s^2)_var'\n",
      " 'waist resultant acceleration_mean' 'target']\n"
     ]
    }
   ],
   "source": [
    "dfResWindows = filterCols(df2_all_windows,['Acceleration'],['waist'],True)\n",
    "dfResWindows['target'] = dfResWindows.apply(lambda row: generateTarget2(row),axis=1)\n",
    "print(dfResWindows.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "df2List = []\n",
    "for sub in dfResWindows['subject'].unique() :\n",
    "    for trialtype in dfResWindows['trial_type'].unique() :\n",
    "        for subtype in dfResWindows['trial_subtype'].unique() :\n",
    "            for num in dfResWindows['trial_num'].unique() :\n",
    "                aux1 = dfResWindows[(dfResWindows['subject'] == sub) & \n",
    "                                         (dfResWindows['trial_type'] == trialtype) & \n",
    "                                         (dfResWindows['trial_subtype'] == subtype) & \n",
    "                                         (dfResWindows['trial_num'] == num)]\n",
    "                if (aux1.shape[0] > 0) :\n",
    "                    if (trialtype == 'Falls') :\n",
    "                        peak_index = aux1['waist resultant acceleration_mean'].idxmax()\n",
    "                        for i in range(peak_index-4,peak_index+4) : # Add the target 1 to the window\n",
    "                            aux1.set_value(i, 'target', 1)\n",
    "                    df2List.append(aux1)\n",
    "\n",
    "fulldf2 = pd.concat(df2List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      "TN : 8627 FP : 114\n",
      "FN : 509 TP : 331\n",
      "\n",
      "Accuracy : 0.934975472289\n",
      "Recall : 0.394047619048\n",
      "Precision : 0.743820224719\n",
      "F-measure :0.515175097276\n",
      "Sensitivity : 39.4047619048\n",
      "Specificity : 98.6958013957\n"
     ]
    }
   ],
   "source": [
    "y_train = fulldf2[(fulldf2['subject'] >= 6)]['target']\n",
    "X_train = fulldf2[(fulldf2['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "y_test = fulldf2[(fulldf2['subject'] < 6)]['target']\n",
    "X_test = fulldf2[(fulldf2['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Resultant Difference Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the mean\n",
    "df3_window_mean = df3.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='1000000us')]).mean()\n",
    "df3_window_mean = df3_window_mean.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_mean' in the end\n",
    "for col in df3.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df3_window_mean.rename(columns={col: str(col+'_mean')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group in intervals of 0.5 seconds, calculating the variance\n",
    "\n",
    "df3_window_variance = df3.groupby(['subject','trial_type','trial_subtype','trial_num','trial_num_original',pd.Grouper(key='time_datetime', freq='1000000us')]).var()\n",
    "df3_window_variance = df3_window_variance.reset_index()\n",
    "\n",
    "# renaming the acceleration measurement columns, including a '_variance' in the end\n",
    "for col in df3.columns.values :\n",
    "    if ('Acceleration' in col) or ('Velocity' in col) or ('Magnetic' in col) or ('resultant' in col) :\n",
    "        df3_window_variance.rename(columns={col: str(col+'_var')}, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe, with all accelerometer columns (means and variances)\n",
    "df3_all_windows = pd.merge(df3_window_mean, df3_window_variance,on=['subject', 'trial_type', 'trial_subtype', 'trial_num','trial_num_original','time_datetime'])\n",
    "\n",
    "# This dataframe will be used in case we decide to try different preprocessing steps\n",
    "df3_all_windows = df3_all_windows.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject' 'trial_type' 'trial_subtype' 'trial_num' 'trial_num_original'\n",
      " 'time_datetime' 'waist Acceleration Z (m/s^2)_var'\n",
      " 'waist Acceleration X (m/s^2)_mean' 'waist resultant acceleration_var'\n",
      " 'waist Acceleration X (m/s^2)_var' 'waist Acceleration Y (m/s^2)_var'\n",
      " 'waist Acceleration Y (m/s^2)_mean' 'waist Acceleration Z (m/s^2)_mean'\n",
      " 'waist resultant acceleration_mean' 'target']\n"
     ]
    }
   ],
   "source": [
    "df3ResWindows = filterCols(df3_all_windows,['Acceleration'],['waist'],True)\n",
    "df3ResWindows['target'] = df3ResWindows.apply(lambda row: generateTarget2(row),axis=1)\n",
    "print(df3ResWindows.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "df3List = []\n",
    "for sub in df3ResWindows['subject'].unique() :\n",
    "    for trialtype in df3ResWindows['trial_type'].unique() :\n",
    "        for subtype in df3ResWindows['trial_subtype'].unique() :\n",
    "            for num in df3ResWindows['trial_num'].unique() :\n",
    "                aux1 = df3ResWindows[(df3ResWindows['subject'] == sub) & \n",
    "                                         (df3ResWindows['trial_type'] == trialtype) & \n",
    "                                         (df3ResWindows['trial_subtype'] == subtype) & \n",
    "                                         (df3ResWindows['trial_num'] == num)]\n",
    "                aux1['resultant_diff'] = df3ResWindows['waist resultant acceleration_mean'].diff().fillna(0)\n",
    "                df3List.append(aux1)\n",
    "\n",
    "df3resdiff3 = pd.concat(df3List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject' 'trial_type' 'trial_subtype' 'trial_num' 'trial_num_original'\n",
      " 'time_datetime' 'waist Acceleration Z (m/s^2)_var'\n",
      " 'waist Acceleration X (m/s^2)_mean' 'waist resultant acceleration_var'\n",
      " 'waist Acceleration X (m/s^2)_var' 'waist Acceleration Y (m/s^2)_var'\n",
      " 'waist Acceleration Y (m/s^2)_mean' 'waist Acceleration Z (m/s^2)_mean'\n",
      " 'waist resultant acceleration_mean' 'target' 'resultant_diff']\n"
     ]
    }
   ],
   "source": [
    "print(df3resdiff3.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "# This takes a while to run (about at least 10-15mins)\n",
    "df3List = []\n",
    "for sub in df3resdiff3['subject'].unique() :\n",
    "    for trialtype in df3resdiff3['trial_type'].unique() :\n",
    "        for subtype in df3resdiff3['trial_subtype'].unique() :\n",
    "            for num in df3resdiff3['trial_num'].unique() :\n",
    "                aux1 = df3resdiff3[(df3resdiff3['subject'] == sub) & \n",
    "                                         (df3resdiff3['trial_type'] == trialtype) & \n",
    "                                         (df3resdiff3['trial_subtype'] == subtype) & \n",
    "                                         (df3resdiff3['trial_num'] == num)]\n",
    "                if (aux1.shape[0] > 0) :\n",
    "                    if (trialtype == 'Falls') :\n",
    "                        peak_index = aux1['resultant_diff'].idxmax()\n",
    "                        for i in range(peak_index-2,peak_index+2) : # Add the target 1 to the window\n",
    "                            aux1.set_value(i, 'target', 1)\n",
    "                    df3List.append(aux1)\n",
    "\n",
    "fulldf3 = pd.concat(df3List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      "TN : 4455 FP : 65\n",
      "FN : 210 TP : 210\n",
      "\n",
      "Accuracy : 0.944331983806\n",
      "Recall : 0.5\n",
      "Precision : 0.763636363636\n",
      "F-measure :0.604316546763\n",
      "Sensitivity : 50.0\n",
      "Specificity : 98.5619469027\n"
     ]
    }
   ],
   "source": [
    "y_train = fulldf3[(fulldf3['subject'] >= 6)]['target']\n",
    "X_train = fulldf3[(fulldf3['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "y_test = fulldf3[(fulldf3['subject'] < 6)]['target']\n",
    "X_test = fulldf3[(fulldf3['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo', cache_size=500000, coef0=0, C=1, gamma=0.01,  class_weight=None)\n",
    "modelProcessing(X_train,y_train,X_test,y_test,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject' 'trial_type' 'trial_subtype' 'trial_num' 'trial_num_original'\n",
      " 'time_datetime' 'waist Acceleration Y (m/s^2)'\n",
      " 'waist Acceleration Z (m/s^2)' 'waist Acceleration X (m/s^2)'\n",
      " 'waist resultant acceleration' 'target']\n"
     ]
    }
   ],
   "source": [
    "df4 = filterCols(df4,['Acceleration'],['waist'],True)\n",
    "df4['target'] = df4.apply(lambda row: generateTarget2(row),axis=1)\n",
    "print(df4.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the window for each subject,trialtype, subtype and number and combine them all into one single dataframe\n",
    "df4List = []\n",
    "for sub in df4['subject'].unique() :\n",
    "    for trialtype in df4['trial_type'].unique() :\n",
    "        for subtype in df4['trial_subtype'].unique() :\n",
    "            for num in df4['trial_num'].unique() :\n",
    "                aux1 = df4[(df4['subject'] == sub) & \n",
    "                                         (df4['trial_type'] == trialtype) & \n",
    "                                         (df4['trial_subtype'] == subtype) & \n",
    "                                         (df4['trial_num'] == num)]\n",
    "                if (aux1.shape[0] > 0) :\n",
    "                    if (trialtype == 'Falls') :\n",
    "                        peak_index = aux1['waist resultant acceleration'].idxmax()\n",
    "                        for i in range(peak_index-256,peak_index+256) : # Add the target 1 to the window\n",
    "                            aux1.set_value(i, 'target', 1)\n",
    "                    df4List.append(aux1)\n",
    "\n",
    "fulldf4 = pd.concat(df4List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = fulldf4[(fulldf4['subject'] >= 6)]['target']\n",
    "X_train = fulldf4[(fulldf4['subject'] >= 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)\n",
    "\n",
    "y_test = fulldf4[(fulldf4['subject'] < 6)]['target']\n",
    "X_test = fulldf4[(fulldf4['subject'] < 6)].drop(['trial_num_original',\n",
    "                'trial_type', 'subject', 'trial_subtype', \n",
    "                    'trial_num','target','time_datetime'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D,MaxPooling1D, Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_9_input to have shape (None, 10, 1) but got array with shape (595502, 4, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e0c295aee846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1410\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_9_input to have shape (None, 10, 1) but got array with shape (595502, 4, 1)"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "conv = Sequential()\n",
    "conv.add(Conv1D(filters=4, kernel_size=1, input_shape = X_train.shape[1:4], activation = 'relu'))\n",
    "conv.add(MaxPooling1D(4))\n",
    "conv.add(Flatten())\n",
    "conv.add(Dense(1, activation = 'sigmoid'))\n",
    "sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv.fit(X_train, y_train, batch_size = 500, epochs = 50, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.302806179657\n",
      "Accuracy : 0.909895825453\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "metrics = conv.evaluate(X_test,y_test,verbose=0)\n",
    "print(\"Loss : \"+str(metrics[0]))\n",
    "print(\"Accuracy : \"+str(metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp,tn,fp,fn = 0,0,0,0\n",
    "i = 0\n",
    "while i < X_test.shape[0] :\n",
    "    elem = X_test[i:i+4]\n",
    "    pred = conv.predict(elem)\n",
    "    realVal = y_test[i:i+4]\n",
    "    \n",
    "    # Real value\n",
    "    totalReal = 0\n",
    "    y = -1\n",
    "    for val in realVal : \n",
    "        totalReal += val\n",
    "    totalReal = totalReal / 4\n",
    "    if totalReal >= 0.5 : \n",
    "        y = 1\n",
    "    else : \n",
    "        y = 0\n",
    "        \n",
    "    # Predicted value\n",
    "    total = 0\n",
    "    predVal = -1\n",
    "    for num in pred : \n",
    "        total += num\n",
    "    total = total / 4\n",
    "    if (total >= 0.5) :\n",
    "        predVal = 1\n",
    "    else : \n",
    "        predVal = 0\n",
    "        \n",
    "    if (y == 1) and (predVal == 1) :\n",
    "        tp += 1\n",
    "    elif (y == 0) and (predVal == 0) :\n",
    "        tn += 1\n",
    "    elif (y == 1) and (predVal == 0) :\n",
    "        fn += 1\n",
    "    elif (y == 0) and (predVal == 1) :\n",
    "        fp += 1\n",
    "     \n",
    "    i += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      "TN : 135292 FP : 0\n",
      "FN : 13425 TP : 0\n",
      "\n",
      "Accuracy : 0.9097278724019446\n",
      "Sensitivity : 0.0\n",
      "Specificity : 100.0\n"
     ]
    }
   ],
   "source": [
    "specificity = (tn / (fp + tn))*100\n",
    "sensitivity = (tp / (tp + fn))*100\n",
    "accuracy = ((tn+tp) / (tp + tn + fp + fn))\n",
    "print(\"Confusion matrix : \")\n",
    "print(\"TN : \"+str(tn) + \" FP : \" +str(fp))\n",
    "print(\"FN : \"+str(fn) + \" TP : \" +str(tp))\n",
    "print(\"\")\n",
    "print(\"Accuracy : \"+str(accuracy))\n",
    "print(\"Sensitivity : \"+str(sensitivity))\n",
    "print(\"Specificity : \"+str(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
